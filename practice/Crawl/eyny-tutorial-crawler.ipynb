{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##package\n",
    "\n",
    "import requests\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "### function\n",
    "\n",
    "def getAuthor(table):\n",
    "    authorEle   = table.select(\"tr\")[0].select(\"td\")[0]\n",
    "    author_name = authorEle.select(\"div.authi a\")[0].text\n",
    "    tdTag       = table.select(\"tr\")[0].select(\"td\")[0]\n",
    "    \n",
    "    author_post_cnt     = tdTag.select(\"dl.pil dd\")[0].text\n",
    "    author_score        = tdTag.select(\"dl.pil dd\")[1].text.replace(\" 點\",\"\")\n",
    "    author_diving_value = tdTag.select(\"dl.pil dd\")[2].text.replace(\" 米\",\"\")\n",
    "    \n",
    "    return {\n",
    "        \"author_name\" : author_name,\n",
    "        \"author_post_cnt\"     : int(author_post_cnt),\n",
    "        \"author_score\"        : int(author_score),\n",
    "        \"author_diving_value\" : int(author_diving_value)\n",
    "    }\n",
    "\n",
    "def getContent(table):\n",
    "    trTag = table.select(\"tr\")[1]\n",
    "    if trTag.i: trTag.i.extract()\n",
    "    return trTag.text\n",
    "\n",
    "def getTime(table):\n",
    "    return table.select(\"td\")[1].select(\"div.authi em\")[0].text.replace(\"發表於 \",\"\").replace(\" AM\",\"\")\n",
    "\n",
    "\n",
    "######################################\n",
    "def getHiddenParams(url):\n",
    "    \n",
    "    res0  = requests.get(url)\n",
    "    soup0 = bs(res0.text,\"lxml\")\n",
    "\n",
    "    cookietime = soup0.select(\"input[name='cookietime']\")[0][\"value\"]\n",
    "    formhash   = soup0.select(\"input[name='formhash']\")[0][\"value\"]\n",
    "    # print(formhash)\n",
    "    \n",
    "    \n",
    "    return {\"cookietime\" : cookietime, \"formhash\" : formhash}\n",
    "\n",
    "######################################\n",
    "def login(formhash, cookietime):\n",
    "    \n",
    "    loginUrl = \"https://www.eyny.com/member.php?mod=logging&action=login&loginsubmit=yes&loginhash=LIhj1&inajax=1\"\n",
    "\n",
    "    payload = {\n",
    "        \"formhash\"            : formhash,\n",
    "        \"referer\"             : \"https://www.eyny.com/thread-12290829-1-GU7Y04C7.html\",\n",
    "        \"loginfield\"          : \"username\",\n",
    "        \"username\"            : \"7003un\",\n",
    "        \"password\"            : \"0939771419\",\n",
    "        \"questionid\"          : \"0\",\n",
    "        \"answer\"              : \"\",\n",
    "        \"cookietime\"          : cookietime,\n",
    "        \"g-recaptcha-response\": \"03AOLTBLSQ9n63zcqqPQQA5FCEohXZtKD76G2DoV6_HNsggOVi4BBFl7lxBDZLlCL3kyXWYbAIiRBuXN6L9e_qk1c5-R1B4noiWkEwx_IrKRP8Oshlp5N-P_J0EoikEfTMlrRjjLy59ox7PRyxoYWR9UN5cRG_5BT7iSGeeSJagwB1YZ710EbM9rqJKDrMnDzMtv9q6aL7omX0sWCu07SNZO7r81kInV8DhD7F3AMdchIZ8Y6TH-hvRUIOHnVNWG4yDo0m1TvtyUwHnDuVjq-d6sGJfSUGouoOabss2InPrhp321m79N6RUGM\"\n",
    "    }\n",
    "\n",
    "    loginHeaders = {\n",
    "        \"Host\": \"www.eyny.com\",\n",
    "        \"Origin\": \"https://www.eyny.com\",\n",
    "        \"Referer\": \"https://www.eyny.com/member.php?mod=logging&action=login\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    sess = requests.session()\n",
    "    res = sess.post(loginUrl , headers = loginHeaders , data = payload)\n",
    "    # print(res.text)\n",
    "    # print(res.cookies.get_dict())\n",
    "    \n",
    "    return sess\n",
    "######################################\n",
    "\n",
    "\n",
    "def crawlArticleList(sess):\n",
    "\n",
    "    # Step 2\n",
    "    ### 取得文章列表\n",
    "    baseUrl = \"https://www.eyny.com/\"\n",
    "    forum = \"forum-1710-1.html\"\n",
    "    links = []\n",
    "\n",
    "    headers = {\n",
    "        \"Host\": \"www.eyny.com\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
    "    }\n",
    "    res  = sess.get(baseUrl+forum , headers = headers)\n",
    "    soup = bs(res.text , \"lxml\") \n",
    "\n",
    "    subLinks = [ baseUrl+ele[\"href\"] for ele in soup.select(\"div#threadlist table[summary] th.new a.xst\")]\n",
    "    links += subLinks\n",
    "\n",
    "\n",
    "    # links\n",
    "    nextLinks = [baseUrl+a[\"href\"] for a in soup.select(\"div.pg a\")[1:10]]\n",
    "    lastLink  = baseUrl+forum\n",
    "\n",
    "    for nextLink in nextLinks:\n",
    "        headers[\"Refer\"] = lastLink\n",
    "\n",
    "        res  = sess.get(nextLink , headers = headers)\n",
    "        subSoup = bs(res.text , \"lxml\") \n",
    "\n",
    "        subLinks = [ baseUrl+ele[\"href\"] for ele in subSoup.select(\"div#threadlist table[summary] th.common a.xst\")]\n",
    "        links += subLinks\n",
    "\n",
    "        lastLink = nextLink\n",
    "        \n",
    "    return links\n",
    "\n",
    "######################################\n",
    "def crawlArticleDetail(sess,links):\n",
    "\n",
    "    dataList = []\n",
    "    for l in links:\n",
    "        headers = {\n",
    "        \"Host\": \"www.eyny.com\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
    "    }\n",
    "              \n",
    "        try:\n",
    "    #     res2  = requests.get(l,headers=headers)\n",
    "            res2  = sess.get(l,headers=headers)\n",
    "            soup2 = bs(res2.text,\"lxml\")\n",
    "            tableHeader = soup2.select(\"div#postlist > table\")[0]\n",
    "\n",
    "            pageview = tableHeader.select(\"td\")[0].select(\"span\")[1].text\n",
    "            resp_cnt = tableHeader.select(\"td\")[0].select(\"span\")[4].text\n",
    "            title    = tableHeader.select(\"td\")[1].text.replace(\"[複製鏈接]\",\"\")\n",
    "\n",
    "            tables = soup2.select(\"div#postlist > div > table\")\n",
    "            mainTable = tables[0]\n",
    "\n",
    "            author   = getAuthor(mainTable)\n",
    "            content  = getContent(mainTable)\n",
    "            time     = getTime(mainTable)\n",
    "\n",
    "            data = {\n",
    "                \"title\" : title,\n",
    "                \"time\"  : time,\n",
    "                \"content\" : content,\n",
    "                \"pageview\": pageview,\n",
    "                \"resp_cnt\": resp_cnt,\n",
    "                \"author_name\"          : author[\"author_name\"],\n",
    "                \"author_post_cnt\"      : author[\"author_post_cnt\"],\n",
    "                \"author_score\"         : author[\"author_score\"],\n",
    "                \"author_diving_value\"  : author[\"author_diving_value\"],\n",
    "                \"resp\": []\n",
    "            }\n",
    "\n",
    "            for table in tables[1:]:\n",
    "                respAuthor = getAuthor(table)\n",
    "                resp_data = {\n",
    "                    \"time\"                : getTime(table),\n",
    "                    \"content\"             : getContent(table),\n",
    "                    \"author_name\"         : respAuthor[\"author_name\"],\n",
    "                    \"author_post_cnt\"     : respAuthor[\"author_post_cnt\"],\n",
    "                    \"author_score\"        : respAuthor[\"author_score\"],\n",
    "                    \"author_diving_value\" : respAuthor[\"author_diving_value\"],\n",
    "                }\n",
    "\n",
    "                data[\"resp\"].append(resp_data)\n",
    "\n",
    "\n",
    "            dataList.append(data)\n",
    "\n",
    "            ### 檢查是否文章有效\n",
    "            print(data[\"title\"])\n",
    "            print(\"=\"*80)\n",
    "        except:\n",
    "            print(\"*\"*80)\n",
    "            traceback.print_exc()\n",
    "            print(\"*\"*80)\n",
    "            \n",
    "    return dataList\n",
    "######################################\n",
    "\n",
    "def saveFile(dataList,name):\n",
    "\n",
    "    df = pd.DataFrame(dataList)\n",
    "    df = df[[\"title\",\"time\",\"resp_cnt\",\"pageview\",\"content\",\"author_name\",\"author_post_cnt\",\"author_score\",\"author_diving_value\",\"resp\"]]\n",
    "    df.to_excel(name,index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "### main    \n",
    "# Step 0\n",
    "### 取得隱藏變數\n",
    "loginPageUrl = \"https://www.eyny.com/member.php?mod=logging&action=login\"   \n",
    "hiddenParams = getHiddenParams(loginPageUrl)\n",
    "print(\"Step 0 : 取得隱藏變數 ok !!!\")\n",
    "\n",
    "\n",
    "## Step 1\n",
    "### 登入\n",
    "\n",
    "sess = login(hiddenParams[\"formhash\"], hiddenParams[\"cookietime\"])\n",
    "print(\"Step 1 : 登入 ok !!!\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 2\n",
    "    ### 取得文章列表\n",
    "links = crawlArticleList(sess)    \n",
    "print(\"Step 2 : 文章列表 ok !!!\")\n",
    "\n",
    "\n",
    "\n",
    "# Step3\n",
    "### 抓取文章內容\n",
    "# l = links[8]\n",
    "\n",
    "dataList = crawlArticleDetail(sess,links[:10])\n",
    "\n",
    "print(\"Step 3 : 抓取文章內容 ok !!!\")\n",
    "\n",
    "\n",
    "# Step4\n",
    "### 存資料\n",
    "\n",
    "name= \"20190810-sample.xlsx\"\n",
    "saveFile(dataList, name)\n",
    "print(\"Step 4 : 存成 excel ok !!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
